{"title":"Kinesis Data Firehose","slug":"kinesisDataFirehose","date":"2020-05-01T01:21:23.000Z","updated":"2020-11-23T01:31:25.294Z","comments":true,"path":"api/articles/kinesisDataFirehose.json","photos":[],"link":"","excerpt":"","covers":["https://user-images.githubusercontent.com/62123161/80773866-2b896000-8b96-11ea-9292-4988675db895.png","https://user-images.githubusercontent.com/62123161/80773864-2b896000-8b96-11ea-863b-97cf308afdc9.png","https://user-images.githubusercontent.com/62123161/80773862-2a583300-8b96-11ea-8f42-d5299b504504.png"],"content":"<h2 id=\"개발환경-구축\"><a href=\"#개발환경-구축\" class=\"headerlink\" title=\"개발환경 구축\"></a>개발환경 구축</h2><p>※ 기본 개발환경</p>\n<ul>\n<li>IntelliJ</li>\n<li>Java, Spring</li>\n<li>AWS Console</li>\n</ul>\n<h2 id=\"Data-Firehose와-Data-Stream과-다른점\"><a href=\"#Data-Firehose와-Data-Stream과-다른점\" class=\"headerlink\" title=\"Data Firehose와 Data Stream과 다른점\"></a>Data Firehose와 Data Stream과 다른점</h2><ol>\n<li>Kinesis Stream은 기본 수준 서비스로, 각 파티션이 내부적으로 주문 된 여러 리더를 지원하는 파티션 된 데이터 스트림. 병합 / 분할 샤드를 관리하여 운영해야 함.</li>\n<li>커스터마이징이 가능하며 맞춤형 애플리케이션을 구축하거나 특수한 요구에 맞게 데이터를 스트리밍하는 개발자에게 가장 적합합니다. 그러나 수동 조정 및 프로비저닝이 필요합니다. 데이터는 일반적으로 24 시간 동안 스트림으로 제공되지만 추가 비용을 지불하면 최대 7 일 동안 데이터 가용성을 얻을 수 있습니다.</li>\n<li>Firehose는 매우 많은 Kinesis 사용 사례를 위해 구축 된 Kinesis 스트림 위에 구축 된 서비스로, 많은 데이터를 수집하고 이를 S3 / Redshift / ElasticSearch와 같은 스토리지 솔루션으로 파이핑 함. 파티션을 관리.</li>\n<li>스케일링은 초당 최대 기가 바이트까지 자동으로 처리되며 배치, 암호화 및 압축이 가능합니다.<br><a href=\"https://user-images.githubusercontent.com/62123161/80773866-2b896000-8b96-11ea-9292-4988675db895.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://user-images.githubusercontent.com/62123161/80773866-2b896000-8b96-11ea-9292-4988675db895.png\" alt=\"kinesisfirehose_01\"></a></li>\n</ol>\n<h3 id=\"1-Firehose-생성\"><a href=\"#1-Firehose-생성\" class=\"headerlink\" title=\"1) Firehose 생성\"></a>1) Firehose 생성</h3><p><a href=\"https://user-images.githubusercontent.com/62123161/80773864-2b896000-8b96-11ea-863b-97cf308afdc9.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://user-images.githubusercontent.com/62123161/80773864-2b896000-8b96-11ea-863b-97cf308afdc9.png\" alt=\"kinesisfirehose_02\"></a><br>① Firehose Name 입력<br>② 자동으로 Put되는 방식인지, DataStream에서 받아서 전달하는 방식인지 선택.<br>    현재 우리 서비스에서는 Direct Put 해도 상관없음. Data를 특정 Stream에서 컨트롤해야한다면 Stream을 선택해주면 됨.<br>③ CMK를 활용하여 Stream의 평문데이터를 Firehose로 가져오면서 암호화 진행여부<br><a href=\"https://user-images.githubusercontent.com/62123161/80773863-2af0c980-8b96-11ea-9b59-4bcd47295594.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://user-images.githubusercontent.com/62123161/80773863-2af0c980-8b96-11ea-9b59-4bcd47295594.png\" alt=\"kinesisfirehose_03\"></a><br>④ Lambda를 통해 저장할 데이터를 손볼건지 결정하는 부분.<br>⑤ AWS Glue를 통해 전처리 작업을 진행할지 선택하는 부분.</p>\n<p> *Glue 란?<br>AWS Glue는 완전 관리형 ETL(추출, 변환, 로드) 서비스로, 효율적인 비용으로 간단하게 여러 데이터 스토어 간에 원하는 데이터를 분류, 정리, 보강, 이동합니다. AWS Glue는 AWS Glue 데이터 카탈로그으로 알려진 중앙 메타데이터 리포지토리, 자동적으로 Python 및 Scala 코드를 생성하는 ETL 엔진 및 종속적 해결 방안, 작업 모니터링 및 재시도를 관리하는 유연성 스케줄러로 구성됩니다. AWS Glue는 서버리스이므로 설정하거나 관리할 인프라가 없습니다.<br><a href=\"https://user-images.githubusercontent.com/62123161/80773862-2a583300-8b96-11ea-8f42-d5299b504504.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://user-images.githubusercontent.com/62123161/80773862-2a583300-8b96-11ea-8f42-d5299b504504.png\" alt=\"kinesisfirehose_04\"></a><br>⑥ S3 / Redshift / Elasticsearch Service / Splunk 등 AWS의 목적지로 보낼 곳 선택<br>     (우리 서비스에서는 우선 S3를 사용함. 추후 ES, Splunk 등 검토)<br>⑦ prefix / error prefix (저장되는 파일의 S3 디렉토리 지정)<br>     kd/year-!{timestamp:YYYY}/month-!{timestamp:MM}/day-!{timestamp:dd}/<br>     kderror/!{firehose:random-string}/!{firehose:error-output-type}/!{timestamp:yyyy/MM/dd}/<br><a href=\"https://user-images.githubusercontent.com/62123161/80773857-288e6f80-8b96-11ea-87e2-9efc8190f38c.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://user-images.githubusercontent.com/62123161/80773857-288e6f80-8b96-11ea-87e2-9efc8190f38c.png\" alt=\"kinesisfirehose_05\"></a><br>⑧ Buffer사이즈와 주기를 정해주어야 함.<br>    SmartTok 기준 40000대 15분동안 45MB가 쌓이고 있음. 용량이 부족하면 꽉 찬 데이터까지만 보내므로 조절이 필요함.(매우중요)<br>⑨ Compression(압축방식지정.. 비용문제만 아니면 안해도됨)<br>⑩ S3 Encryption<br>    KMS를 통한 암호화. 3번의 CMK를 통한 암호화를 지정해도되고 저장할때 KMS를 통해 지정해도 됨. 암호화는 나중에 반영.<br>⑪ Logging<br>    로깅은 항상 Enable함.<br>    본인이 코딩의 신이거나 AWS의 지배자가 아니라면 로깅 및 디버깅은 생활화 하자.<br>⑫ Permission(권한)<br>    권한은 기본권한으로..(AWS 사상 중 가장 중요시되는 최소권한의 원칙)</p>\n<h3 id=\"2-Data-직접-넣기\"><a href=\"#2-Data-직접-넣기\" class=\"headerlink\" title=\"2) Data 직접 넣기?\"></a>2) Data 직접 넣기?</h3><ul>\n<li><p>코딩이 없어서 아쉬우셨다면, 잠시 코딩을 갖는 시간을 갖겠음.</p>\n</li>\n<li><p>솔직히 Firehose는 AWS 완전관리형이기에 크게 손볼게 없다.</p>\n</li>\n<li><p>단순히 데이터 넣는 API정도뿐이라 간단히 소개함.</p>\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"BASH\"><figure class=\"iseeu highlight bash\"><figcaption><span>basic AWS Credential</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">BasicAWSCredentials awsCreds = new BasicAWSCredentials(<span class=\"string\">\"ACCESS_KEY\"</span>,</span><br><span class=\"line\">            <span class=\"string\">\"SECRET_KEY\"</span>);</span><br><span class=\"line\">    AmazonKinesisFirehoseClientBuilder clientBuilder = AmazonKinesisFirehoseClientBuilder.standard();</span><br><span class=\"line\">    clientBuilder.setRegion(<span class=\"string\">\"ap-northeast-2\"</span>); // 서울 region</span><br><span class=\"line\">    clientBuilder.setCredentials(new AWSStaticCredentialsProvider(awsCreds));</span><br><span class=\"line\">    AmazonKinesisFirehose kinesisClient = clientBuilder.build();</span><br></pre></td></tr></table></figure></div>\n\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"BASH\"><figure class=\"iseeu highlight bash\"><figcaption><span>single write</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//단일 쓰기 작업</span><br><span class=\"line\">static void sendData(AmazonKinesisFirehose kinesisClient, int count) &#123;</span><br><span class=\"line\">    String myData = <span class=\"string\">\"&#123;\\\"no\\\":\"</span> + count + <span class=\"string\">\"&#125;\\n\"</span>; // 보내려는 데이터</span><br><span class=\"line\">    PutRecordRequest putRecordsRequest = new PutRecordRequest();</span><br><span class=\"line\">    putRecordsRequest.setDeliveryStreamName(STREAM_NAME);</span><br><span class=\"line\">    Record rc = new Record();</span><br><span class=\"line\">    rc.setData(ByteBuffer.wrap(myData.getBytes()));</span><br><span class=\"line\">    putRecordsRequest.setRecord(rc);</span><br><span class=\"line\">    PutRecordResult putRecordsResult = kinesisClient.putRecord(putRecordsRequest);</span><br><span class=\"line\">    System.out.println(<span class=\"string\">\"Put Result\"</span> + putRecordsResult);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></div>\n\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"BASH\"><figure class=\"iseeu highlight bash\"><figcaption><span>multiple write</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//////////////////////////////////////////</span><br><span class=\"line\"> // 10번 반복해서 전송(일괄전송)</span><br><span class=\"line\">        HashSet&lt;Record&gt; recordList = new HashSet&lt;Record&gt;();</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (int i = 0; i &lt; 10 ; i++) &#123;</span><br><span class=\"line\">            recordList.add(createData());</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        multiSendData(kinesisClient, recordList);</span><br><span class=\"line\">//////////////////////////////////////////</span><br><span class=\"line\"> </span><br><span class=\"line\">    //일괄 쓰기 작업</span><br><span class=\"line\">    static void multiSendData(AmazonKinesisFirehose kinesisClient, HashSet&lt;Record&gt; recordList)&#123;</span><br><span class=\"line\">        PutRecordBatchRequest putRecordBatchRequest = new PutRecordBatchRequest();</span><br><span class=\"line\">        putRecordBatchRequest.setDeliveryStreamName(STREAM_NAME);</span><br><span class=\"line\">        putRecordBatchRequest.setRecords(recordList);</span><br><span class=\"line\"> </span><br><span class=\"line\">        // Put Record Batch records. Max No.Of Records we can put <span class=\"keyword\">in</span> a</span><br><span class=\"line\">        // single put record batch request is 500</span><br><span class=\"line\">        PutRecordBatchResult putRecordBatchResult = kinesisClient.putRecordBatch(putRecordBatchRequest);</span><br><span class=\"line\"> </span><br><span class=\"line\">        recordList.clear();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    public static Record <span class=\"function\"><span class=\"title\">createData</span></span>() &#123;</span><br><span class=\"line\">        String data = ThreadLocalRandom.current().nextInt() + <span class=\"string\">\"\\n\"</span>;</span><br><span class=\"line\">        <span class=\"built_in\">return</span> new Record().withData(ByteBuffer.wrap(data.getBytes()));</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure></div>\n\n<h3 id=\"3-현재-사용처\"><a href=\"#3-현재-사용처\" class=\"headerlink\" title=\"3) 현재 사용처\"></a>3) 현재 사용처</h3><ul>\n<li>AWS IoT Core 내의 Rule Engine에 현재 사용중임. DynamoDB를 대체해서 사용하고자 하며, 추후 Analytics를 통하거나 해서 MySQL에 저장할 예정.</li>\n</ul>\n</li>\n<li><p>저장기간이 최대 7일이 한계이므로, 주기적으로 Batch작업 돌려주어야 할 것으로 보임.</p>\n<h3 id=\"결론\"><a href=\"#결론\" class=\"headerlink\" title=\"결론\"></a>결론</h3></li>\n<li><p>솔직히 Data Stream의 활용범위는 상당히 넓다. 기본적인 SQS기능뿐만 아니라 AWS 다른 솔루션 플랫폼에 적용할 수 있을정도로 활용범위가 넓은 점은 장점이지만, 모든 설정을 해줘야 한다는 단점이 있다.</p>\n</li>\n<li><p>Firehose는 몇개 안되는 설정으로 자동으로 원하는 기능을 제공해주니 단순비교는 어렵지만 개인적으로 쓰기 편한감이 있음.</p>\n</li>\n<li><p>(코딩이 필요없다…….. 위 대제목 2번에 쓴 코드를 쓸 일 자체가 없을 것으로 보임.)</p>\n</li>\n</ul>\n","categories":[{"name":"aws","slug":"aws","count":19,"path":"api/categories/aws.json"}],"tags":[{"name":"AWS","slug":"AWS","count":20,"path":"api/tags/AWS.json"},{"name":"Kinesis","slug":"Kinesis","count":4,"path":"api/tags/Kinesis.json"},{"name":"firehose","slug":"firehose","count":1,"path":"api/tags/firehose.json"}]}